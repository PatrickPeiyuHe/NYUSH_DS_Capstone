{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Peak the power spectrum of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import mne\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the folder containing the .fif files\n",
    "processed_folder = \"AllData/Preprocessed_FIF_OpenNeuroHC\"  # Replace with the actual path to the folder\n",
    "\n",
    "# Define the folder for PSD plots\n",
    "psd_plot_folder = os.path.join(processed_folder, \"PSD_Plot\")\n",
    "os.makedirs(psd_plot_folder, exist_ok=True)  # Create the folder if it doesn't exist\n",
    "\n",
    "# Loop through all .fif files in the folder\n",
    "for file_name in os.listdir(processed_folder):\n",
    "    if file_name.endswith(\".fif\"):  # Process only .fif files\n",
    "        file_path = os.path.join(processed_folder, file_name)\n",
    "\n",
    "        # Load the .fif file\n",
    "        print(f\"Processing: {file_name}\")\n",
    "        raw = mne.io.read_raw_fif(file_path, preload=True)\n",
    "\n",
    "        # Plot the PSD for each channel\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        raw.plot_psd(fmax=45, show=False)  # Limit frequency range to 50 Hz for clarity\n",
    "        plt.title(f\"PSD for {file_name}\")\n",
    "\n",
    "        # Save the plot to the PSD_Plot folder\n",
    "        output_plot_path = os.path.join(psd_plot_folder, f\"{file_name.replace('.fif', '_psd.png')}\")\n",
    "        plt.savefig(output_plot_path)\n",
    "        plt.close()\n",
    "\n",
    "        print(f\"PSD plot saved for {file_name} at {output_plot_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decomposing periodic and aperiodic components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import mne\n",
    "import numpy as np\n",
    "from fooof import FOOOF\n",
    "import pandas as pd\n",
    "\n",
    "# Define the folder containing the .fif files\n",
    "processed_folder = \"AllData/preprocessed_dryad\"  # Replace with your path\n",
    "\n",
    "# Initialize data storage\n",
    "results = []\n",
    "\n",
    "# Loop through all .fif files in the folder\n",
    "for file_name in os.listdir(processed_folder):\n",
    "    if file_name.endswith(\".fif\"):  # Process only .fif files\n",
    "        file_path = os.path.join(processed_folder, file_name)\n",
    "\n",
    "        # Load the .fif file\n",
    "        raw = mne.io.read_raw_fif(file_path, preload=True)\n",
    "\n",
    "        # Compute PSD for each channel\n",
    "        psd = raw.compute_psd(fmin=1, fmax=45, n_fft=1024, verbose=False)\n",
    "        freqs = psd.freqs  # Extract frequencies\n",
    "        psds = psd.get_data()  # Power values for each channel\n",
    "\n",
    "        # Loop through each channel to apply FOOOF\n",
    "        for ch_idx, ch_name in enumerate(raw.ch_names):\n",
    "            channel_psd = psds[ch_idx]  # PSD for the current channel\n",
    "\n",
    "            # Initialize FOOOF model\n",
    "            fm = FOOOF(aperiodic_mode='knee',max_n_peaks=7,min_peak_height=0.1)  # Use 'knee' mode\n",
    "\n",
    "            # Fit FOOOF to the PSD\n",
    "            fm.fit(freqs, channel_psd)\n",
    "            # fm.plot()\n",
    "            # Collect aperiodic and periodic parameters\n",
    "            aperiodic_params = fm.aperiodic_params_\n",
    "            peak_params = fm.peak_params_\n",
    "\n",
    "            # Append the results for this channel\n",
    "            results.append({\n",
    "                'Subject': file_name,\n",
    "                'Channel': ch_name,\n",
    "                'Aperiodic_Offset': aperiodic_params[0],\n",
    "                'Aperiodic_Exponent': aperiodic_params[1],\n",
    "                'Aperiodic_Knee': aperiodic_params[2] if len(aperiodic_params) > 2 else None,\n",
    "                'Peak_Params': peak_params.tolist(),\n",
    "            })\n",
    "\n",
    "# Save results to a CSV file\n",
    "results_df = pd.DataFrame(results)\n",
    "output_path = os.path.join(processed_folder, \"fooof_results_channels.csv\")\n",
    "results_df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"FOOOF analysis completed and results saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Coherence features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.signal import coherence\n",
    "import mne\n",
    "\n",
    "# Define the folder containing the .fif files\n",
    "processed_folder = \"AllData/preprocessed_dryad\"  # Replace with your folder path\n",
    "\n",
    "# Define frequency bands of interest\n",
    "bands = {\n",
    "    \"Delta\": (1, 4),\n",
    "    \"Theta\": (4, 8),\n",
    "    \"Alpha\": (8, 12),\n",
    "    \"Beta\": (12, 30),\n",
    "    \"Gamma\": (30, 45),\n",
    "}\n",
    "\n",
    "# Initialize the DataFrame to store results\n",
    "results = []\n",
    "\n",
    "# Parameters for coherence calculation\n",
    "fs = 250  # Sampling frequency (after downsampling)\n",
    "\n",
    "# Loop through each .fif file in the folder\n",
    "for file_name in os.listdir(processed_folder):\n",
    "    if file_name.endswith(\".fif\"):  # Process only .fif files\n",
    "        file_path = os.path.join(processed_folder, file_name)\n",
    "        \n",
    "        # Load the .fif file\n",
    "        raw = mne.io.read_raw_fif(file_path, preload=True)\n",
    "        data, channel_names = raw.get_data(), raw.ch_names\n",
    "        \n",
    "        # Calculate coherence between every pair of channels\n",
    "        n_channels = len(channel_names)\n",
    "        for i in range(n_channels):\n",
    "            for j in range(i + 1, n_channels):  # Avoid duplicate pairs\n",
    "                # Extract the two signals\n",
    "                signal_1, signal_2 = data[i, :], data[j, :]\n",
    "                \n",
    "                # Calculate coherence\n",
    "                freqs, coh_values = coherence(signal_1, signal_2, fs=fs, nperseg=fs * 2)  # Adjust nperseg for segments\n",
    "                \n",
    "                # Loop through each frequency band\n",
    "                for band_name, (fmin, fmax) in bands.items():\n",
    "                    # Extract average coherence in the frequency band of interest\n",
    "                    band_indices = (freqs >= fmin) & (freqs <= fmax)\n",
    "                    avg_coh = np.mean(coh_values[band_indices])\n",
    "                    \n",
    "                    # Append results to the list\n",
    "                    results.append({\n",
    "                        \"Subject\": file_name,\n",
    "                        \"Channel_Pair\": f\"{channel_names[i]}-{channel_names[j]}\",\n",
    "                        \"Band\": band_name,\n",
    "                        \"Coherence\": avg_coh\n",
    "                    })\n",
    "\n",
    "# Convert results to a DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Save the results to a CSV file\n",
    "output_path = os.path.join(processed_folder, \"coherence_results.csv\")\n",
    "results_df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Coherence calculation completed and saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entropy features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import mne\n",
    "import pandas as pd\n",
    "from antropy import spectral_entropy  # Or use other entropy functions like 'perm_entropy', 'svd_entropy'\n",
    "\n",
    "# Path to the folder containing .fif files\n",
    "processed_fif_folder = 'AllData/Figshare/Processed_FIF'  # Replace with your folder path\n",
    "\n",
    "# Initialize a list to store combined results\n",
    "combined_results = []\n",
    "\n",
    "# Loop through all .fif files in the folder\n",
    "for file_name in os.listdir(processed_fif_folder):\n",
    "    if file_name.endswith('.fif'):  # Only process .fif files\n",
    "        file_path = os.path.join(processed_fif_folder, file_name)\n",
    "        \n",
    "        # Load the .fif file\n",
    "        print(f\"Processing: {file_name}\")\n",
    "        raw = mne.io.read_raw_fif(file_path, preload=True)\n",
    "        \n",
    "        # Get the data (channels x time) and sampling frequency\n",
    "        data, sfreq = raw.get_data(), raw.info['sfreq']\n",
    "        \n",
    "        # Calculate entropy for each channel\n",
    "        for i, channel_name in enumerate(raw.ch_names):\n",
    "            channel_data = data[i, :]  # Data for one channel\n",
    "            entropy = spectral_entropy(channel_data, sfreq, method='fft')  # Compute spectral entropy\n",
    "            combined_results.append({\n",
    "                'Subject': file_name.replace('.fif', ''),  # Extract subject ID\n",
    "                'Channel': channel_name,\n",
    "                'Entropy': entropy\n",
    "            })\n",
    "\n",
    "# Convert combined results to a DataFrame\n",
    "entropy_df = pd.DataFrame(combined_results)\n",
    "\n",
    "# Save the combined results to a CSV file\n",
    "output_path = os.path.join(processed_fif_folder, 'all_entropy_features.csv')\n",
    "entropy_df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Combined entropy features saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I didn't use this feature, but I onced look at it, not that helpful compared with the other 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import mne\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.signal import hilbert\n",
    "\n",
    "# Path to the folder containing .fif files\n",
    "processed_fif_folder = 'AllData/Figshare/Processed_FIF'  # Update with your folder path\n",
    "\n",
    "# Initialize a list to store results\n",
    "results = []\n",
    "\n",
    "# Loop through all .fif files in the folder\n",
    "for file_name in os.listdir(processed_fif_folder):\n",
    "    if file_name.endswith('.fif'):  # Only process .fif files\n",
    "        file_path = os.path.join(processed_fif_folder, file_name)\n",
    "        \n",
    "        # Load the .fif file\n",
    "        print(f\"Processing: {file_name}\")\n",
    "        raw = mne.io.read_raw_fif(file_path, preload=True)\n",
    "        \n",
    "        # Get the data (channels x time) and sampling frequency\n",
    "        data, sfreq = raw.get_data(), raw.info['sfreq']\n",
    "        \n",
    "        # Calculate Fluctuate Dynamics and Amplitude Envelope Variance for each channel\n",
    "        for i, channel_name in enumerate(raw.ch_names):\n",
    "            channel_data = data[i, :]  # Data for one channel\n",
    "            \n",
    "            # Compute the analytic signal using the Hilbert transform\n",
    "            analytic_signal = hilbert(channel_data)\n",
    "            \n",
    "            # Extract the amplitude envelope\n",
    "            amplitude_envelope = np.abs(analytic_signal)\n",
    "            \n",
    "            # Compute Fluctuate Dynamics as the standard deviation of the amplitude envelope\n",
    "            fluctuate_dynamics = np.std(amplitude_envelope)\n",
    "            \n",
    "            # Compute Amplitude Envelope Variance\n",
    "            envelope_variance = np.var(amplitude_envelope)\n",
    "            \n",
    "            # Append the results\n",
    "            results.append({\n",
    "                'Subject': file_name.replace('.fif', ''),  # Extract subject ID\n",
    "                'Channel': channel_name,\n",
    "                'Fluctuate_Dynamics': fluctuate_dynamics,\n",
    "                'Amplitude_Envelope_Variance': envelope_variance\n",
    "            })\n",
    "\n",
    "# Convert results to a DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Save results to a CSV file\n",
    "output_path = os.path.join(processed_fif_folder, 'oscillation_fluctuation_features.csv')\n",
    "results_df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Oscillation features saved to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# File path\n",
    "file_path = 'AllData/Figshare/Processed_FIF/oscillation_fluctuation_features.csv'  # Replace with the actual file path\n",
    "\n",
    "# Load the file\n",
    "features_df = pd.read_csv(file_path)\n",
    "\n",
    "# Reshape the data: pivot to create one row per subject with 19*2 features\n",
    "pivot_df = features_df.pivot(index='Subject', columns='Channel', values=['Fluctuate_Dynamics', 'Amplitude_Envelope_Variance'])\n",
    "\n",
    "# Flatten the multi-index columns to single-level columns with format \"channel_FD\" or \"channel_AEV\"\n",
    "pivot_df.columns = [f\"{channel}_{metric}\" for metric, channel in pivot_df.columns]\n",
    "\n",
    "# Reset the index to make it a DataFrame\n",
    "pivot_df.reset_index(inplace=True)\n",
    "\n",
    "# Create labels (assuming subject names starting with 'H' are healthy, and others are MDD)\n",
    "pivot_df['Target'] = pivot_df['Subject'].apply(lambda x: 0 if x.startswith('H') else 1)\n",
    "\n",
    "# Separate features and target\n",
    "X = pivot_df.drop(columns=['Subject', 'Target'])\n",
    "y = pivot_df['Target']\n",
    "groups = pivot_df['Subject']  # Use subjects for group-based cross-validation\n",
    "\n",
    "# Initialize Random Forest model\n",
    "rf_model = RandomForestClassifier(random_state=42, n_estimators=100)\n",
    "\n",
    "# Perform subject-wise cross-validation using GroupKFold\n",
    "group_kfold = GroupKFold(n_splits=10)\n",
    "cv_scores = []\n",
    "\n",
    "# Run subject-wise cross-validation\n",
    "for train_idx, test_idx in group_kfold.split(X, y, groups):\n",
    "    # Split the data\n",
    "    X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "    y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "    \n",
    "    # Train the Random Forest model\n",
    "    rf_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict on the test set\n",
    "    y_pred = rf_model.predict(X_test)\n",
    "    \n",
    "    # Compute accuracy\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    cv_scores.append(acc)\n",
    "\n",
    "# Calculate mean and standard deviation of the scores\n",
    "mean_accuracy = np.mean(cv_scores)\n",
    "std_accuracy = np.std(cv_scores)\n",
    "\n",
    "# Print results\n",
    "print(f\"10-Fold Subject-wise Cross-Validation Accuracy Scores: {cv_scores}\")\n",
    "print(f\"Mean Accuracy: {mean_accuracy:.4f} ± {std_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature get-together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the datasets\n",
    "entropy_path = 'AllData/Figshare/Processed_FIF/all_entropy_features.csv'\n",
    "coherence_path = 'AllData/Figshare/Processed_FIF/coherence_results.csv'\n",
    "aperiodic_path = 'AllData/Figshare/Processed_FIF/fooof_results_channels.csv'\n",
    "\n",
    "# Read the data\n",
    "entropy_df = pd.read_csv(entropy_path)\n",
    "coherence_df = pd.read_csv(coherence_path)\n",
    "aperiodic_df = pd.read_csv(aperiodic_path)\n",
    "\n",
    "# Clean subject names by removing \".fif\"\n",
    "entropy_df['Subject'] = entropy_df['Subject'].str.replace('.fif', '', regex=False)\n",
    "coherence_df['Subject'] = coherence_df['Subject'].str.replace('.fif', '', regex=False)\n",
    "aperiodic_df['Subject'] = aperiodic_df['Subject'].str.replace('.fif', '', regex=False)\n",
    "\n",
    "# Prepare Entropy Features\n",
    "entropy_pivot = entropy_df.pivot(index='Subject', columns='Channel', values='Entropy')\n",
    "entropy_pivot.columns = [f\"{col}_Entropy\" for col in entropy_pivot.columns]\n",
    "entropy_pivot.reset_index(inplace=True)\n",
    "\n",
    "# Prepare Coherence Features\n",
    "coherence_df['Feature'] = coherence_df['Channel_Pair'] + \"_\" + coherence_df['Band']\n",
    "coherence_pivot = coherence_df.pivot(index='Subject', columns='Feature', values='Coherence')\n",
    "coherence_pivot.columns = [f\"{col}_Coherence\" for col in coherence_pivot.columns]\n",
    "coherence_pivot.reset_index(inplace=True)\n",
    "\n",
    "# Prepare Aperiodic Features\n",
    "aperiodic_pivot = aperiodic_df.pivot(index='Subject', columns='Channel', values=['Aperiodic_Offset', 'Aperiodic_Exponent', 'Aperiodic_Knee'])\n",
    "aperiodic_pivot.columns = ['_'.join(col).strip() for col in aperiodic_pivot.columns.values]\n",
    "aperiodic_pivot.reset_index(inplace=True)\n",
    "\n",
    "# Merge all features into one DataFrame\n",
    "merged_df = pd.merge(entropy_pivot, coherence_pivot, on='Subject', how='outer')\n",
    "merged_df = pd.merge(merged_df, aperiodic_pivot, on='Subject', how='outer')\n",
    "\n",
    "# Save the combined DataFrame\n",
    "output_path = 'AllData/Figshare/Processed_FIF/all_combined_features.csv'\n",
    "merged_df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Combined features saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the combined dataset\n",
    "file_path = 'AllData/Figshare/Processed_FIF/all_combined_features.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Extract the group label from the Subject column\n",
    "data['Group'] = data['Subject'].apply(lambda x: 'H' if x.startswith('H') else 'MDD')\n",
    "\n",
    "# Function to fill missing values with group-specific mean\n",
    "def fill_missing_with_group_mean(df, group_col, value_cols):\n",
    "    filled_df = df.copy()\n",
    "    # Replace infinite values with NaN\n",
    "    for col in value_cols:\n",
    "        filled_df[col] = filled_df[col].replace([float('inf'), float('-inf')], pd.NA)\n",
    "    # Fill NaN values with group-specific mean\n",
    "    for col in value_cols:\n",
    "        filled_df[col] = filled_df.groupby(group_col)[col].transform(lambda group: group.fillna(group.mean()))\n",
    "    return filled_df\n",
    "\n",
    "# Identify feature columns (excluding Subject and Group columns)\n",
    "feature_columns = data.columns.difference(['Subject', 'Group'])\n",
    "\n",
    "# Fill missing values\n",
    "filled_data = fill_missing_with_group_mean(data, group_col='Group', value_cols=feature_columns)\n",
    "\n",
    "# Save the filled dataset\n",
    "output_path = 'AllData/Figshare/Processed_FIF/all_combined_features_filled.csv'\n",
    "filled_data.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Filled dataset saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the combined dataset\n",
    "file_path = 'AllData/Figshare/Processed_FIF/all_combined_features.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Extract the group label from the Subject column\n",
    "data['Group'] = data['Subject'].apply(lambda x: 'H' if x.startswith('H') else 'MDD')\n",
    "\n",
    "# Function to fill missing values with group-specific mean and normalize data\n",
    "def fill_and_normalize_group_mean(df, group_col, value_cols):\n",
    "    filled_df = df.copy()\n",
    "    \n",
    "    # Replace infinite values with NaN\n",
    "    for col in value_cols:\n",
    "        filled_df[col] = filled_df[col].replace([float('inf'), float('-inf')], pd.NA)\n",
    "    \n",
    "    # Fill NaN values with group-specific mean\n",
    "    for col in value_cols:\n",
    "        filled_df[col] = filled_df.groupby(group_col)[col].transform(lambda group: group.fillna(group.mean()))\n",
    "    \n",
    "    # Normalize the features\n",
    "    scaler = StandardScaler()\n",
    "    filled_df[value_cols] = scaler.fit_transform(filled_df[value_cols])\n",
    "    \n",
    "    return filled_df\n",
    "\n",
    "# Identify feature columns (excluding Subject and Group columns)\n",
    "feature_columns = data.columns.difference(['Subject', 'Group'])\n",
    "\n",
    "# Fill missing values and normalize features\n",
    "filled_data = fill_and_normalize_group_mean(data, group_col='Group', value_cols=feature_columns)\n",
    "\n",
    "# Save the filled and normalized dataset\n",
    "output_path = 'AllData/Figshare/Processed_FIF/all_combined_features_filled_normalized.csv'\n",
    "filled_data.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Filled and normalized dataset saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the filled dataset\n",
    "file_path = 'AllData/Figshare/Processed_FIF/all_combined_features_filled_normalized.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Replace infinite values with NaN\n",
    "data.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Drop rows with any remaining NaN values\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "# Check and ensure all columns are numeric (convert if necessary)\n",
    "for col in data.columns:\n",
    "    if col not in ['Subject', 'Group']:\n",
    "        data[col] = pd.to_numeric(data[col], errors='coerce')\n",
    "\n",
    "# Drop any rows with remaining NaNs after conversion\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "# Separate features, target, and groups\n",
    "X = data.drop(columns=['Subject', 'Group'])\n",
    "y = data['Group'].apply(lambda x: 0 if x == 'H' else 1)  # Binary target: 0 for 'H', 1 for 'MDD'\n",
    "groups = data['Subject']\n",
    "\n",
    "# Normalize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Initialize Random Forest model with best parameters\n",
    "rf_model = RandomForestClassifier(\n",
    "    max_depth=None, \n",
    "    max_features='log2', \n",
    "    min_samples_leaf=1, \n",
    "    min_samples_split=2, \n",
    "    n_estimators=100, \n",
    "    random_state=22\n",
    ")\n",
    "\n",
    "# Recursive Feature Elimination (RFE) for top 25 features\n",
    "rfe = RFE(estimator=rf_model, n_features_to_select=25, step=10)\n",
    "X_selected = rfe.fit_transform(X_scaled, y)\n",
    "\n",
    "# Get the selected feature names\n",
    "selected_features = X.columns[rfe.support_].tolist()\n",
    "print(f\"Selected Top 25 Features: {selected_features}\")\n",
    "\n",
    "# Perform subject-wise cross-validation\n",
    "group_kfold = GroupKFold(n_splits=10)\n",
    "cv_scores = []\n",
    "\n",
    "for train_idx, test_idx in group_kfold.split(X_selected, y, groups):\n",
    "    # Split the data\n",
    "    X_train, X_test = X_selected[train_idx], X_selected[test_idx]\n",
    "    y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "    \n",
    "    # Train Random Forest model\n",
    "    rf_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict and evaluate\n",
    "    y_pred = rf_model.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    cv_scores.append(acc)\n",
    "\n",
    "# Calculate mean and standard deviation of the scores\n",
    "mean_accuracy = np.mean(cv_scores)\n",
    "std_accuracy = np.std(cv_scores)\n",
    "\n",
    "print(f\"10-Fold Subject-wise Cross-Validation Accuracy Scores: {cv_scores}\")\n",
    "print(f\"Mean Accuracy: {mean_accuracy:.4f} ± {std_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Visualizing Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Extract feature importance scores from the trained Random Forest model\n",
    "feature_importances = pd.DataFrame({\n",
    "    'Feature': selected_features,  # Top 25 selected feature names\n",
    "    'Importance': rf_model.feature_importances_\n",
    "})\n",
    "\n",
    "# Sort features by importance\n",
    "feature_importances = feature_importances.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Plot the feature importance scores\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(feature_importances['Feature'], feature_importances['Importance'], color='tab:blue')\n",
    "plt.xlabel('Importance Score', fontsize=12)\n",
    "plt.ylabel('Features', fontsize=12)\n",
    "plt.title('Feature Importance (Top 25 Selected Features)', fontsize=14)\n",
    "plt.gca().invert_yaxis()  # Show the most important features on top\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Visualizing Feature Importance 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define brain regions and feature types\n",
    "brain_regions = {\n",
    "    'frontal_left': ['Fp1', 'F3', 'F7'],\n",
    "    'frontal_right': ['Fp2', 'F4', 'F8'],\n",
    "    'temporal_left': ['T3', 'T5'],\n",
    "    'temporal_right': ['T4', 'T6'],\n",
    "    'central_left': ['C3'],\n",
    "    'central_right': ['C4'],\n",
    "    'parietal_left': ['P3'],\n",
    "    'parietal_right': ['P4'],\n",
    "    'occipital_left': ['O1'],\n",
    "    'occipital_right': ['O2']\n",
    "}\n",
    "\n",
    "\n",
    "feature_types = {\n",
    "    'aperiodic_offset': 'Offset',\n",
    "    'aperiodic_knee': 'Knee',\n",
    "    'coherence': 'Coherence'\n",
    "}\n",
    "\n",
    "# Initialize dictionaries to store weighted sums\n",
    "region_weights = {region: 0 for region in brain_regions.keys()}\n",
    "feature_type_weights = {ftype: 0 for ftype in feature_types.keys()}\n",
    "\n",
    "# Corrected handling of coherence features\n",
    "for _, row in feature_importances.iterrows():\n",
    "    feature_name = row['Feature']\n",
    "    importance = row['Importance']\n",
    "    \n",
    "    # Check for feature type\n",
    "    for ftype, identifier in feature_types.items():\n",
    "        if identifier in feature_name:\n",
    "            if ftype == 'coherence':\n",
    "                # Split coherence features into channels and weight them\n",
    "                channels = feature_name.split('_')[0:2]  # Extract channel names\n",
    "                weight = importance / 2  # Each channel gets half weight\n",
    "                feature_type_weights[ftype] += importance  # Add importance to coherence\n",
    "                \n",
    "                # Assign importance to brain regions\n",
    "                for channel in channels:\n",
    "                    for region, channels_list in brain_regions.items():\n",
    "                        if channel in channels_list:\n",
    "                            region_weights[region] += weight\n",
    "            else:\n",
    "                feature_type_weights[ftype] += importance\n",
    "\n",
    "            # Assign importance for non-coherence features to brain regions\n",
    "            for region, channels_list in brain_regions.items():\n",
    "                if any(channel in feature_name for channel in channels_list):\n",
    "                    region_weights[region] += importance\n",
    "\n",
    "# Print corrected results\n",
    "print(\"Feature Type Importance Weights:\")\n",
    "print(feature_type_weights)\n",
    "print(\"\\nBrain Region Importance Weights:\")\n",
    "print(region_weights)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Colors for feature types\n",
    "feature_type_colors = {\n",
    "    'aperiodic_offset': '#66c2a5',  # Greenish shade\n",
    "    'aperiodic_knee': '#fc8d62',    # Orange shade\n",
    "    'coherence': '#8da0cb'          # Bluish shade\n",
    "}\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Updated color palette with purple shades\n",
    "feature_type_colors = {\n",
    "    'aperiodic_offset': '#a6bddb',  # Light blue-gray\n",
    "    'aperiodic_knee': '#3690c0',    # Medium blue-gray\n",
    "    'coherence': '#8c6bb1'          # Purple\n",
    "}\n",
    "\n",
    "brain_region_colors = {\n",
    "    'frontal_left': '#c7e9c0',   # Soft green\n",
    "    'frontal_right': '#c7e9c0',  # Soft green\n",
    "    'temporal_left': '#9e9ac8',  # Light purple\n",
    "    'temporal_right': '#9e9ac8', # Light purple\n",
    "    'central_left': '#3690c0',   # Medium blue-gray\n",
    "    'central_right': '#3690c0',  # Medium blue-gray\n",
    "    'parietal_left': '#bdbdbd',  # Light gray\n",
    "    'parietal_right': '#bdbdbd', # Light gray\n",
    "    'occipital_left': '#6a51a3', # Dark purple\n",
    "    'occipital_right': '#6a51a3' # Dark purple\n",
    "}\n",
    "\n",
    "# Sorting the weights for ranking\n",
    "sorted_feature_type_weights = {k: v for k, v in sorted(feature_type_weights.items(), key=lambda item: item[1], reverse=True)}\n",
    "sorted_region_weights = {k: v for k, v in sorted(region_weights.items(), key=lambda item: item[1], reverse=True)}\n",
    "\n",
    "# Plot Feature Type Importance\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.pie(sorted_feature_type_weights.values(),\n",
    "        labels=sorted_feature_type_weights.keys(),\n",
    "        colors=[feature_type_colors[key] for key in sorted_feature_type_weights.keys()],\n",
    "        autopct='%1.1f%%',\n",
    "        startangle=140)\n",
    "plt.title('Feature Type Importance', fontsize=14)\n",
    "\n",
    "# Plot Brain Region Importance\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.pie(sorted_region_weights.values(),\n",
    "        labels=sorted_region_weights.keys(),\n",
    "        colors=[brain_region_colors[key] for key in sorted_region_weights.keys()],\n",
    "        autopct='%1.1f%%',\n",
    "        startangle=140)\n",
    "plt.title('Brain Region Importance', fontsize=14)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Grib search best params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV, GroupKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'AllData/Figshare/Processed_FIF/all_combined_features_filled_normalized.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Preprocess the dataset\n",
    "data.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "selected_features = [\n",
    "    'C3-T3_Alpha_Coherence', 'F7-O2_Gamma_Coherence', 'Fp1-F4_Theta_Coherence', 'O2-T4_Alpha_Coherence', 'P4-Cz_Alpha_Coherence', 'Aperiodic_Offset_C3', 'Aperiodic_Offset_C4', 'Aperiodic_Offset_Fp2', 'Aperiodic_Offset_P3', 'Aperiodic_Offset_P4', 'Aperiodic_Offset_Pz', 'Aperiodic_Offset_T4', 'Aperiodic_Offset_T5', 'Aperiodic_Offset_T6', 'Aperiodic_Knee_C3', 'Aperiodic_Knee_Cz', 'Aperiodic_Knee_Fz', 'Aperiodic_Knee_O1', 'Aperiodic_Knee_O2', 'Aperiodic_Knee_P3', 'Aperiodic_Knee_P4', 'Aperiodic_Knee_Pz', 'Aperiodic_Knee_T4', 'Aperiodic_Knee_T5', 'Aperiodic_Knee_T6'\n",
    "\n",
    "]\n",
    "X = data[selected_features]\n",
    "y = data['Group'].apply(lambda x: 0 if x == 'H' else 1)  # Binary target: 0 for 'H', 1 for 'MDD'\n",
    "groups = data['Subject']\n",
    "\n",
    "# Normalize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Apply PCA to reduce dimensionality\n",
    "pca = PCA(n_components=3)  # Adjust number of components as needed\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Initialize GroupKFold\n",
    "group_kfold = GroupKFold(n_splits=10)\n",
    "\n",
    "# Define parameter grids for each model\n",
    "param_grids = {\n",
    "    'RandomForest': {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_depth': [None, 10, 20],\n",
    "        'min_samples_split': [2, 5, 10]\n",
    "    },\n",
    "    'KNN': {\n",
    "        'n_neighbors': [3, 5, 7],\n",
    "        'weights': ['uniform', 'distance'],\n",
    "        'p': [1, 2]\n",
    "    },\n",
    "    'LogisticRegression': {\n",
    "        'C': [0.1, 1, 10],\n",
    "        'max_iter': [100, 200, 500],\n",
    "        'penalty': ['l2'],\n",
    "        'solver': ['lbfgs']\n",
    "    },\n",
    "    'GradientBoosting': {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'learning_rate': [0.01, 0.1, 0.2],\n",
    "        'max_depth': [3, 5, 7]\n",
    "    },\n",
    "    'SVM': {\n",
    "        'C': [0.1, 1, 10],\n",
    "        'kernel': ['linear', 'rbf'],\n",
    "        'gamma': ['scale', 'auto']\n",
    "    }\n",
    "}\n",
    "\n",
    "# Define models\n",
    "models = {\n",
    "    'RandomForest': RandomForestClassifier(random_state=42),\n",
    "    'KNN': KNeighborsClassifier(),\n",
    "    'LogisticRegression': LogisticRegression(random_state=42),\n",
    "    'GradientBoosting': GradientBoostingClassifier(random_state=42),\n",
    "    'SVM': SVC(random_state=42)\n",
    "}\n",
    "\n",
    "# Perform grid search for each model\n",
    "best_params = {}\n",
    "for model_name, model in models.items():\n",
    "    print(f\"\\n--- Grid Search for {model_name} ---\")\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=model,\n",
    "        param_grid=param_grids[model_name],\n",
    "        cv=group_kfold.split(X_pca, y, groups),\n",
    "        scoring='accuracy',\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    grid_search.fit(X_pca, y)\n",
    "    best_params[model_name] = grid_search.best_params_\n",
    "    print(f\"Best Parameters for {model_name}: {grid_search.best_params_}\")\n",
    "    print(f\"Best Accuracy for {model_name}: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "print(\"\\nSummary of Best Parameters:\")\n",
    "for model_name, params in best_params.items():\n",
    "    print(f\"{model_name}: {params}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA and Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "# Define the folder path where the logs are stored\n",
    "folder_path = \"AllData/Figshare/Processed_FIF\"\n",
    "\n",
    "# File paths for the logs\n",
    "rejected_subjects_file = os.path.join(folder_path, \"rejected_subjects.txt\")\n",
    "rejected_channels_file = os.path.join(folder_path, \"rejected_channels.txt\")\n",
    "rejected_components_file = os.path.join(folder_path, \"rejected_components.txt\")\n",
    "\n",
    "# Read the logs\n",
    "with open(rejected_subjects_file, 'r') as f:\n",
    "    rejected_subjects = f.readlines()\n",
    "\n",
    "with open(rejected_channels_file, 'r') as f:\n",
    "    rejected_channels = [line.strip() for line in f.readlines()]\n",
    "\n",
    "with open(rejected_components_file, 'r') as f:\n",
    "    rejected_components = [line.strip() for line in f.readlines()]\n",
    "\n",
    "# Initialize data storage for groups\n",
    "data = {\"H\": {\"subjects\": 0, \"channels\": [], \"independent components\": [], \"variance_explained\": []},\n",
    "        \"MDD\": {\"subjects\": 0, \"channels\": [], \"independent components\": [], \"variance_explained\": []}}\n",
    "\n",
    "# Process rejected subjects\n",
    "for subject in rejected_subjects:\n",
    "    group = \"H\" if subject.startswith(\"H\") else \"MDD\"\n",
    "    data[group][\"subjects\"] += 1\n",
    "\n",
    "# Process rejected channels\n",
    "for line in rejected_channels:\n",
    "    if line:\n",
    "        parts = line.split(\": \")\n",
    "        group = \"H\" if parts[0].startswith(\"H\") else \"MDD\"\n",
    "        channels_list = eval(parts[1]) if len(parts) > 1 and parts[1] else []\n",
    "        data[group][\"channels\"].append(len(channels_list))\n",
    "\n",
    "# Process rejected independent components\n",
    "for line in rejected_components:\n",
    "    if line:\n",
    "        parts = line.split(\": \", 1)\n",
    "        group = \"H\" if parts[0].startswith(\"H\") else \"MDD\"\n",
    "        stats_dict = eval(parts[1]) if len(parts) > 1 else {}\n",
    "        data[group][\"independent components\"].append(stats_dict.get('rejected_components', 0))\n",
    "        data[group][\"variance_explained\"].append(stats_dict.get('variance_explained', 0))\n",
    "\n",
    "# Calculate means and standard errors\n",
    "def calc_stats(data_list):\n",
    "    return np.mean(data_list), np.std(data_list) / np.sqrt(len(data_list))\n",
    "\n",
    "stats = {group: {\n",
    "    \"subjects\": data[group][\"subjects\"],\n",
    "    \"channels\": calc_stats(data[group][\"channels\"]),\n",
    "    \"independent components\": calc_stats(data[group][\"independent components\"]),\n",
    "    \"variance_explained\": calc_stats(data[group][\"variance_explained\"]),\n",
    "} for group in data.keys()}\n",
    "\n",
    "# Perform statistical tests\n",
    "p_channels = ttest_ind(data[\"H\"][\"channels\"], data[\"MDD\"][\"channels\"], equal_var=False).pvalue\n",
    "p_components = ttest_ind(data[\"H\"][\"independent components\"], data[\"MDD\"][\"independent components\"], equal_var=False).pvalue\n",
    "p_variance = ttest_ind(data[\"H\"][\"variance_explained\"], data[\"MDD\"][\"variance_explained\"], equal_var=False).pvalue\n",
    "\n",
    "# Apply Bonferroni correction\n",
    "p_values = [p_channels, p_components, p_variance]\n",
    "bonferroni_corrected = multipletests(p_values, method='bonferroni')[1]\n",
    "\n",
    "# Print corrected p-values\n",
    "print(\"Bonferroni Corrected p-values:\")\n",
    "print(f\"Channels: {bonferroni_corrected[0]:.3f}\")\n",
    "print(f\"Components: {bonferroni_corrected[1]:.3f}\")\n",
    "print(f\"Variance Explained: {bonferroni_corrected[2]:.3f}\")\n",
    "\n",
    "# Plot the results\n",
    "fig, axes = plt.subplots(1, 4, figsize=(22, 6))\n",
    "\n",
    "# Rejected Subjects\n",
    "axes[0].bar([\"H\", \"MDD\"], [stats[\"H\"][\"subjects\"], stats[\"MDD\"][\"subjects\"]])\n",
    "axes[0].set_title(\"Rejected Subjects\")\n",
    "axes[0].set_ylabel(\"Count\")\n",
    "\n",
    "# Rejected Channels\n",
    "axes[1].bar([\"H\", \"MDD\"], [stats[\"H\"][\"channels\"][0], stats[\"MDD\"][\"channels\"][0]],\n",
    "            yerr=[stats[\"H\"][\"channels\"][1], stats[\"MDD\"][\"channels\"][1]], capsize=5)\n",
    "axes[1].set_title(\"Rejected Channels\")\n",
    "axes[1].set_ylabel(\"Mean Count\")\n",
    "axes[1].text(0.5, max(stats[\"H\"][\"channels\"][0], stats[\"MDD\"][\"channels\"][0]),\n",
    "             f\"p={bonferroni_corrected[0]:.3f}\", ha='center')\n",
    "\n",
    "# Rejected Components\n",
    "axes[2].bar([\"H\", \"MDD\"], [stats[\"H\"][\"independent components\"][0], stats[\"MDD\"][\"independent components\"][0]],\n",
    "            yerr=[stats[\"H\"][\"independent components\"][1], stats[\"MDD\"][\"independent components\"][1]], capsize=5)\n",
    "axes[2].set_title(\"Rejected Independent Components\")\n",
    "axes[2].set_ylabel(\"Mean Count\")\n",
    "axes[2].text(0.5, max(stats[\"H\"][\"independent components\"][0], stats[\"MDD\"][\"independent components\"][0]),\n",
    "             f\"p={bonferroni_corrected[1]:.3f}\", ha='center')\n",
    "\n",
    "# Variance Explained by Rejected Components\n",
    "axes[3].bar([\"H\", \"MDD\"], [stats[\"H\"][\"variance_explained\"][0], stats[\"MDD\"][\"variance_explained\"][0]],\n",
    "            yerr=[stats[\"H\"][\"variance_explained\"][1], stats[\"MDD\"][\"variance_explained\"][1]], capsize=5)\n",
    "axes[3].set_title(\"Variance Explained by Rejected Independent Components\")\n",
    "axes[3].set_ylabel(\"Mean Variance (%)\")\n",
    "axes[3].text(0.5, max(stats[\"H\"][\"variance_explained\"][0], stats[\"MDD\"][\"variance_explained\"][0]),\n",
    "             f\"p={bonferroni_corrected[2]:.3f}\", ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "# Initialize dictionaries to store metrics\n",
    "metrics = {model: {} for model in ['RandomForest', 'KNN', 'LogisticRegression', 'GradientBoosting', 'SVM']}\n",
    "\n",
    "# Models with their best parameters\n",
    "models = {\n",
    "    'RandomForest': RandomForestClassifier(max_depth=None, min_samples_split=2, n_estimators=100),\n",
    "    'KNN': KNeighborsClassifier(n_neighbors=3, p=2, weights='uniform'),\n",
    "    'LogisticRegression': LogisticRegression(C=1, max_iter=100, penalty='l2', solver='lbfgs'),\n",
    "    'GradientBoosting': GradientBoostingClassifier(learning_rate=0.01, max_depth=3, n_estimators=50),\n",
    "    'SVM': SVC(C=1, gamma='auto', kernel='rbf')\n",
    "}\n",
    "\n",
    "# Evaluate each model\n",
    "for model_name, model in models.items():\n",
    "    scores = []\n",
    "    precision, recall, f1 = [], [], []\n",
    "    for train_idx, test_idx in group_kfold.split(X_pca, y, groups):\n",
    "        X_train, X_test = X_pca[train_idx], X_pca[test_idx]\n",
    "        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "        \n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        scores.append(accuracy_score(y_test, y_pred))\n",
    "        precision.append(precision_score(y_test, y_pred))\n",
    "        recall.append(recall_score(y_test, y_pred))\n",
    "        f1.append(f1_score(y_test, y_pred))\n",
    "    \n",
    "    # Store metrics\n",
    "    metrics[model_name]['Accuracy'] = f\"{np.mean(scores):.4f} ± {np.std(scores):.4f}\"\n",
    "    metrics[model_name]['Precision'] = f\"{np.mean(precision):.4f} ± {np.std(precision):.4f}\"\n",
    "    metrics[model_name]['Recall'] = f\"{np.mean(recall):.4f} ± {np.std(recall):.4f}\"\n",
    "    metrics[model_name]['F1-Score'] = f\"{np.mean(f1):.4f} ± {np.std(f1):.4f}\"\n",
    "\n",
    "# Print results\n",
    "for model_name, values in metrics.items():\n",
    "    print(f\"=== {model_name} ===\")\n",
    "    for metric_name, metric_value in values.items():\n",
    "        print(f\"{metric_name}: {metric_value}\")\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import cross_val_predict, cross_val_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize models\n",
    "models = {\n",
    "    \"Random Forest\": rf_model,\n",
    "    \"KNN\": knn,\n",
    "    \"Logistic Regression\": log_reg,\n",
    "    \"Gradient Boosting\": gb_model,\n",
    "    \"SVM\": svm_model\n",
    "}\n",
    "\n",
    "# Initialize storage for metrics\n",
    "metrics_summary = []\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    print(f\"Running 10-Fold CV for {model_name}...\")\n",
    "    \n",
    "    # Perform 10-fold CV\n",
    "    y_pred = cross_val_predict(model, X_pca, y, cv=10)\n",
    "    scores = cross_val_score(model, X_pca, y, cv=10)\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y, y_pred)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "    disp.plot()\n",
    "    plt.title(f\"Confusion Matrix (10-Fold CV) - {model_name}\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Classification Report\n",
    "    report = classification_report(y, y_pred, output_dict=True)\n",
    "    print(f\"Classification Report (10-Fold CV) - {model_name}:\\n\", report)\n",
    "    \n",
    "    # Store metrics for summary\n",
    "    metrics_summary.append({\n",
    "        \"Model\": model_name,\n",
    "        \"Mean Accuracy\": np.mean(scores),\n",
    "        \"Accuracy Std Dev\": np.std(scores),\n",
    "        \"Precision\": report[\"weighted avg\"][\"precision\"],\n",
    "        \"Recall\": report[\"weighted avg\"][\"recall\"],\n",
    "        \"F1-Score\": report[\"weighted avg\"][\"f1-score\"]\n",
    "    })\n",
    "\n",
    "# Convert metrics summary to DataFrame\n",
    "metrics_df = pd.DataFrame(metrics_summary)\n",
    "print(metrics_df)\n",
    "\n",
    "# Visualization of Metrics\n",
    "metrics_df.set_index(\"Model\", inplace=True)\n",
    "metrics_df[[\"Mean Accuracy\", \"Precision\", \"Recall\", \"F1-Score\"]].plot(kind='bar', figsize=(10, 6))\n",
    "plt.title(\"Performance Metrics Comparison Across Models (10-Fold CV)\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "components = np.arange(1, len(explained_variance) + 1)\n",
    "\n",
    "plt.figure(figsize=(8, 2))\n",
    "plt.bar(components, explained_variance, alpha=0.7, color='tab:blue', label='Explained Variance')\n",
    "plt.xlabel('Principal Component')\n",
    "plt.ylabel('Variance Ratio')\n",
    "plt.title('Explained Variance by Principal Components')\n",
    "plt.xticks(components)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "t-test and visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import ttest_ind\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "import numpy as np\n",
    "\n",
    "# Load the data\n",
    "file_path = 'AllData/Figshare/Processed_FIF/all_combined_features_filled.csv'  # Replace with the actual path to your CSV file\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Separate the \"Group\" column\n",
    "group_column = 'Group'  # Column name for the group\n",
    "data_features = data.drop(columns=[group_column])  # Drop the group column to keep only features\n",
    "\n",
    "# Remove non-numeric columns\n",
    "data_features = data_features.select_dtypes(include=[np.number])  # Keep only numeric columns\n",
    "\n",
    "# Drop rows with missing or invalid feature values\n",
    "data = pd.concat([data_features, data[[group_column]]], axis=1).dropna()\n",
    "\n",
    "# Split data by group\n",
    "group1 = data[data[group_column] == 'H']  # Group H\n",
    "group2 = data[data[group_column] == 'MDD']  # Group MDD\n",
    "\n",
    "# Extract feature values for the two groups\n",
    "group1_features = group1.drop(columns=[group_column]).values\n",
    "group2_features = group2.drop(columns=[group_column]).values\n",
    "\n",
    "# Ensure that arrays are numeric\n",
    "group1_features = group1_features.astype(float)\n",
    "group2_features = group2_features.astype(float)\n",
    "\n",
    "# Perform t-tests for all features\n",
    "p_values = []\n",
    "t_statistics = []\n",
    "for feature_idx in range(data_features.shape[1]):\n",
    "    # Extract data for the current feature\n",
    "    feature1 = group1_features[:, feature_idx]\n",
    "    feature2 = group2_features[:, feature_idx]\n",
    "    \n",
    "    # Perform t-test\n",
    "    t_stat, p_val = ttest_ind(\n",
    "        feature1,\n",
    "        feature2,\n",
    "        equal_var=False  # Use Welch's t-test for unequal variance\n",
    "    )\n",
    "    t_statistics.append(t_stat)\n",
    "    p_values.append(p_val)\n",
    "\n",
    "# Apply FDR correction for multiple comparisons\n",
    "rejected, corrected_pvals, _, _ = multipletests(p_values, alpha=0.05, method='fdr_bh')\n",
    "\n",
    "# Prepare results for output\n",
    "results = pd.DataFrame({\n",
    "    'Feature': data_features.columns,\n",
    "    't_statistic': t_statistics,\n",
    "    'p_value': p_values,\n",
    "    'corrected_p_value': corrected_pvals,\n",
    "    'significant': rejected\n",
    "})\n",
    "\n",
    "# Save results to a new CSV file\n",
    "results.to_csv('t_test_results_corrected.csv', index=False)\n",
    "\n",
    "# Print summary of significant features\n",
    "print(\"Summary of significant features:\")\n",
    "print(results[results['significant']])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load your results data\n",
    "results_path = \"t_test_results_corrected.csv\"  # Replace with the correct file path\n",
    "results = pd.read_csv(results_path)\n",
    "\n",
    "# Group your features\n",
    "aperiodic_features = results[results['Feature'].str.contains(\"Aperiodic\", case=False)]\n",
    "entropy_features = results[results['Feature'].str.contains(\"Entropy\", case=False)]\n",
    "coherence_features = results[results['Feature'].str.contains(\"Coherence\", case=False)]\n",
    "\n",
    "# Define a helper function to plot boxplots for groups\n",
    "def plot_group(data, group_name, group1_name='H', group2_name='MDD', group1_data=None, group2_data=None):\n",
    "    significant_features = data[data['significant']]\n",
    "    features = significant_features['Feature']\n",
    "    \n",
    "    # Collect group data for significant features\n",
    "    group1_values = group1_data[features]\n",
    "    group2_values = group2_data[features]\n",
    "    \n",
    "    # Combine into a DataFrame for plotting\n",
    "    combined = pd.concat([\n",
    "        group1_values.melt(var_name=\"Feature\", value_name=\"Value\").assign(Group=group1_name),\n",
    "        group2_values.melt(var_name=\"Feature\", value_name=\"Value\").assign(Group=group2_name)\n",
    "    ])\n",
    "    \n",
    "    # Create the boxplot\n",
    "    plt.figure(figsize=(22, 8))\n",
    "    sns.boxplot(x=\"Feature\", y=\"Value\", hue=\"Group\", data=combined, showmeans=True)\n",
    "    plt.xticks(rotation=45)  # Rotate x-axis labels for better visibility\n",
    "    plt.title(f\"{group_name} Features\")\n",
    "    plt.xlabel(\"Features\")\n",
    "    plt.ylabel(\"Values\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Load your original data\n",
    "data_path = \"AllData/Figshare/Processed_FIF/all_combined_features_filled.csv\"  # Replace with the correct file path\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "# Group separation\n",
    "group_column = 'Group'\n",
    "group1_name = 'H'\n",
    "group2_name = 'MDD'\n",
    "\n",
    "# Extract groups\n",
    "group1_data = data[data[group_column] == group1_name].drop(columns=[group_column])\n",
    "group2_data = data[data[group_column] == group2_name].drop(columns=[group_column])\n",
    "\n",
    "# Convert to numeric to avoid issues\n",
    "group1_data = group1_data.apply(pd.to_numeric, errors='coerce')\n",
    "group2_data = group2_data.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Plot for Aperiodic\n",
    "plot_group(aperiodic_features, \"Aperiodic\", group1_name, group2_name, group1_data, group2_data)\n",
    "\n",
    "# Plot for Entropy\n",
    "plot_group(entropy_features, \"Entropy\", group1_name, group2_name, group1_data, group2_data)\n",
    "\n",
    "# Plot for Coherence\n",
    "plot_group(coherence_features, \"Coherence\", group1_name, group2_name, group1_data, group2_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "\n",
    "# Load the data\n",
    "significant_results_path = 't_test_results_corrected.csv'  # Replace with actual file path\n",
    "all_features_path = 'AllData/Figshare/Processed_FIF/all_combined_features_filled.csv'    # Replace with actual file path\n",
    "\n",
    "significant_results = pd.read_csv(significant_results_path)\n",
    "all_features = pd.read_csv(all_features_path)\n",
    "\n",
    "# Filter significant coherence features\n",
    "significant_coherence = significant_results[\n",
    "    significant_results['Feature'].str.contains('Coherence') &\n",
    "    significant_results['significant'] == True\n",
    "]\n",
    "\n",
    "# Extract bands\n",
    "bands = ['Alpha', 'Beta', 'Delta', 'Gamma', 'Theta']\n",
    "\n",
    "channel_positions = {\n",
    "    'Fp1': (0.2, 0.9),\n",
    "    'Fp2': (0.8, 0.9),\n",
    "    'F7': (0.1, 0.7),\n",
    "    'F3': (0.35, 0.8),\n",
    "    'Fz': (0.5, 0.85),\n",
    "    'F4': (0.65, 0.8),\n",
    "    'F8': (0.9, 0.7),\n",
    "    'T3': (0.05, 0.5),\n",
    "    'C3': (0.35, 0.5),\n",
    "    'Cz': (0.5, 0.5),\n",
    "    'C4': (0.65, 0.5),\n",
    "    'T4': (0.95, 0.5),\n",
    "    'T5': (0.1, 0.3),\n",
    "    'P3': (0.35, 0.2),\n",
    "    'Pz': (0.5, 0.15),\n",
    "    'P4': (0.65, 0.2),\n",
    "    'T6': (0.9, 0.3),\n",
    "    'O1': (0.2, 0.1),\n",
    "    'O2': (0.8, 0.1),\n",
    "}\n",
    "# Function to extract and calculate mean difference\n",
    "def get_band_connectivity(band):\n",
    "    edges = []\n",
    "    for _, row in significant_coherence.iterrows():\n",
    "        feature = row['Feature']\n",
    "        if band in feature:\n",
    "            channels = feature.split('_')[0].split('-')\n",
    "            group_1_mean = all_features.loc[all_features['Subject'].str.startswith('H'), feature].mean()\n",
    "            group_2_mean = all_features.loc[all_features['Subject'].str.startswith('MDD'), feature].mean()\n",
    "            mean_diff = group_2_mean - group_1_mean\n",
    "            edges.append((channels[0], channels[1], mean_diff))\n",
    "    return edges\n",
    "\n",
    "# Function to plot functional connectivity map\n",
    "def plot_connectivity(edges, title):\n",
    "    G = nx.Graph()\n",
    "    for edge in edges:\n",
    "        G.add_edge(edge[0], edge[1], weight=abs(edge[2]))\n",
    "\n",
    "    pos = channel_positions\n",
    "    weights = [G[u][v]['weight'] for u, v in G.edges()]\n",
    "    edge_colors = ['purple' if edge[2] > 0 else 'tab:blue' for edge in edges]  # Red for increase, blue for decrease\n",
    "\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    nx.draw_networkx_nodes(G, pos, node_size=700, node_color='grey')\n",
    "    nx.draw_networkx_edges(\n",
    "        G, pos, edge_color=edge_colors, width=[w * 15 for w in weights], alpha=0.8\n",
    "    )\n",
    "    nx.draw_networkx_labels(G, pos, font_size=8)\n",
    "    plt.title(title, fontsize=10)\n",
    "    plt.show()\n",
    "\n",
    "# Plot functional connectivity maps for each band\n",
    "for band in bands:\n",
    "    band_edges = get_band_connectivity(band)\n",
    "    plot_connectivity(band_edges, f'Functional Connectivity Changes in MDD - {band} Band')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn import datasets, surface, plotting\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Fetch the fsaverage brain template\n",
    "fsaverage = datasets.fetch_surf_fsaverage()\n",
    "\n",
    "# Load the pial surface (left hemisphere as an example)\n",
    "pial_surface = surface.load_surf_mesh(fsaverage['pial_left'])\n",
    "\n",
    "# Define node positions for the 19 channels (10-20 system example)\n",
    "node_positions = {\n",
    "    'Fp1': [-30, 90, 60], 'Fp2': [30, 90, 60], 'F3': [-50, 50, 60], 'F4': [50, 50, 60],\n",
    "    'C3': [-70, 0, 50], 'C4': [70, 0, 50], 'P3': [-50, -50, 60], 'P4': [50, -50, 60],\n",
    "    'O1': [-30, -90, 60], 'O2': [30, -90, 60], 'F7': [-90, 50, 40], 'F8': [90, 50, 40],\n",
    "    'T3': [-100, 0, 20], 'T4': [100, 0, 20], 'T5': [-90, -50, 40], 'T6': [90, -50, 40],\n",
    "    'Fz': [0, 70, 70], 'Cz': [0, 0, 80], 'Pz': [0, -70, 70]\n",
    "}\n",
    "\n",
    "# Define example coherence edges with weights\n",
    "edges = [\n",
    "    ('Fp1', 'Fp2', 0.8), ('F3', 'F4', 0.7), ('C3', 'Cz', 0.6), ('Cz', 'C4', 0.9),\n",
    "    ('P3', 'P4', 0.5), ('O1', 'O2', 0.4), ('F7', 'F8', 0.3), ('T3', 'T4', 0.6)\n",
    "]\n",
    "\n",
    "# Scale weights for visualization\n",
    "edge_weights = [edge[2] for edge in edges]\n",
    "min_weight, max_weight = min(edge_weights), max(edge_weights)\n",
    "normalized_weights = [(weight - min_weight) / (max_weight - min_weight) for weight in edge_weights]\n",
    "\n",
    "# Prepare edge coordinates for plotting\n",
    "edge_coords = []\n",
    "for edge in edges:\n",
    "    src, dest, _ = edge\n",
    "    edge_coords.append((node_positions[src], node_positions[dest]))\n",
    "\n",
    "# Plot the brain surface\n",
    "fig = plt.figure(figsize=(12, 12))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "plotting.plot_surf(pial_surface, axes=ax, cmap='coolwarm', alpha=0.6)\n",
    "\n",
    "# Plot the connectivity\n",
    "for coord, weight in zip(edge_coords, normalized_weights):\n",
    "    src, dest = coord\n",
    "    xs, ys, zs = zip(src, dest)\n",
    "    ax.plot(xs, ys, zs, color=plt.cm.viridis(weight), linewidth=2)\n",
    "\n",
    "# Plot nodes\n",
    "for node, position in node_positions.items():\n",
    "    ax.scatter(*position, color='red', s=50, label=node)\n",
    "\n",
    "ax.set_title(\"3D Functional Connectivity\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flowchart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphviz import Digraph\n",
    "\n",
    "# Create a directed graph\n",
    "dot = Digraph()\n",
    "\n",
    "# Define nodes\n",
    "dot.node(\"Start\", shape=\"ellipse\")\n",
    "dot.node(\"Data Preprocessing\", shape=\"box\")\n",
    "dot.node(\"Feature Extraction\", shape=\"box\")\n",
    "dot.node(\"Feature Standardization\", shape=\"box\")\n",
    "dot.node(\"Feature Selection & Dimensional Reduction\", shape=\"box\")\n",
    "dot.node(\"Model Building & Fine-tuning\", shape=\"box\")\n",
    "dot.node(\"Discussion & Conclusion\", shape=\"box\")\n",
    "dot.node(\"End\", shape=\"ellipse\")\n",
    "\n",
    "# Define edges\n",
    "dot.edges([\n",
    "    (\"Start\", \"Data Preprocessing\"),\n",
    "    (\"Data Preprocessing\", \"Feature Extraction\"),\n",
    "    (\"Feature Extraction\", \"Feature Standardization\"),\n",
    "    (\"Feature Standardization\", \"Feature Selection & Dimensional Reduction\"),\n",
    "    (\"Feature Selection & Dimensional Reduction\", \"Model Building & Fine-tuning\"),\n",
    "    (\"Model Building & Fine-tuning\", \"Discussion & Conclusion\"),\n",
    "    (\"Discussion & Conclusion\", \"End\")\n",
    "])\n",
    "\n",
    "# Render and save as a PNG file\n",
    "dot.render(\"workflow_diagram\", format=\"png\", cleanup=True)\n",
    "\n",
    "# Display the image\n",
    "from IPython.display import Image\n",
    "Image(\"workflow_diagram.png\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
