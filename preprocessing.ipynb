{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Peak raw data info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def peek_fif_info(file_path):\n",
    "    \"\"\"\n",
    "    Loads a .fif file and prints its information.\n",
    "    \n",
    "    Parameters:\n",
    "        file_path (str): Path to the .fif file.\n",
    "    \"\"\"\n",
    "    # Load the .fif file\n",
    "    raw = mne.io.read_raw_fif(file_path, preload=False)\n",
    "    \n",
    "    # Print the dataset info\n",
    "    print(\"\\nDataset Information:\")\n",
    "    print(raw.info)\n",
    "    \n",
    "    # Print channel names\n",
    "    print(\"\\nChannel Names:\")\n",
    "    print(raw.info['ch_names'])\n",
    "    \n",
    "    # Print sampling frequency\n",
    "    print(\"\\nSampling Frequency:\")\n",
    "    print(f\"{raw.info['sfreq']} Hz\")\n",
    "    \n",
    "    # Print recording duration\n",
    "    duration = raw.times[-1]  # Last time point\n",
    "    print(\"\\nRecording Duration:\")\n",
    "    print(f\"{duration:.2f} seconds\")\n",
    "    \n",
    "    # Extract raw data\n",
    "    data = raw.get_data()  # Shape: (n_channels, n_times)\n",
    "\n",
    "    # Calculate statistics\n",
    "    min_val = data.min()\n",
    "    max_val = data.max()\n",
    "    mean_val = data.mean()\n",
    "    std_val = data.std()\n",
    "\n",
    "    print(f\"Data range: {min_val:.2e} to {max_val:.2e}\")\n",
    "    print(f\"Mean value: {mean_val:.2e}\")\n",
    "    print(f\"Standard deviation: {std_val:.2e}\")\n",
    "\n",
    "# Path to your .fif file\n",
    "file_path = \"AllData/Figshare/Converted_FIF/H S1 EC.fif\"\n",
    "\n",
    "# Peek at the info\n",
    "peek_fif_info(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert data into .fif files, set montage, truncate 5 mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Figshare to .fif files\n",
    "\n",
    "import os\n",
    "import mne\n",
    "\n",
    "def convert_ec_edf_to_fif_with_montage_and_scaling(input_folder, output_folder_name=\"Converted_FIF\", montage_name=\"standard_1020\"):\n",
    "    \"\"\"\n",
    "    Converts all files ending with 'EC.edf' in the specified folder to FIF format,\n",
    "    renames channels, drops extraneous channels, sets a standard montage, and scales data to microvolts.\n",
    "\n",
    "    Parameters:\n",
    "        input_folder (str): Path to the folder containing the input files.\n",
    "        output_folder_name (str): Name of the subfolder where FIF files will be saved.\n",
    "        montage_name (str): Name of the standard montage to assign for digitization (default: \"standard_1020\").\n",
    "    \"\"\"\n",
    "    # Create output folder under the input folder\n",
    "    output_folder = os.path.join(input_folder, output_folder_name)\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # Process all files in the input folder\n",
    "    for file_name in os.listdir(input_folder):\n",
    "        if file_name.endswith(\"EO.edf\"):  # Only process files ending with 'EC.edf'\n",
    "            edf_path = os.path.join(input_folder, file_name)\n",
    "            fif_name = file_name.replace(\".edf\", \".fif\")  # Replace .edf with .fif\n",
    "            fif_path = os.path.join(output_folder, fif_name)\n",
    "\n",
    "            print(f\"Processing file: {file_name} -> {fif_name}\")\n",
    "\n",
    "            # Load the EDF file\n",
    "            raw = mne.io.read_raw_edf(edf_path, preload=True)\n",
    "\n",
    "            # Step 1: Rename channels to match standard names (remove suffixes like '-LE' or prefixes like 'EEG')\n",
    "            new_ch_names = [ch.replace('-LE', '').replace('EEG ', '') for ch in raw.ch_names]\n",
    "            raw.rename_channels({old: new for old, new in zip(raw.ch_names, new_ch_names)})\n",
    "\n",
    "            # Step 2: Assign channel types to EEG if necessary\n",
    "            if 'eeg' not in raw.get_channel_types():\n",
    "                raw.set_channel_types({ch: 'eeg' for ch in raw.ch_names})\n",
    "\n",
    "            # Step 3: Load the standard montage\n",
    "            montage_obj = mne.channels.make_standard_montage(montage_name)\n",
    "            expected_channel_names = montage_obj.ch_names\n",
    "\n",
    "            # Step 4: Identify and drop channels not part of the standard montage\n",
    "            channels_to_drop = [ch for ch in raw.ch_names if ch not in expected_channel_names]\n",
    "            if channels_to_drop:\n",
    "                raw.drop_channels(channels_to_drop)\n",
    "                print(f\"Dropped channels not in the {montage_name} system: {channels_to_drop}\")\n",
    "            else:\n",
    "                print(f\"No channels need to be dropped. All channels match the {montage_name} system.\")\n",
    "\n",
    "            # Step 5: Set the montage to the raw data\n",
    "            raw.set_montage(montage_obj, match_case=False, on_missing='ignore')\n",
    "            print(f\"Assigned {montage_name} montage to {file_name}\")\n",
    "\n",
    "            # Step 6: Scale data from volts to microvolts (1 V = 1e6 µV)\n",
    "            raw.apply_function(lambda x: x * 1e6, picks=\"eeg\")\n",
    "            print(\"Data scaled from volts to microvolts.\")\n",
    "\n",
    "            # Save as FIF\n",
    "            raw.save(fif_path, overwrite=True)\n",
    "            print(f\"Saved FIF file: {fif_path}\")\n",
    "\n",
    "    print(f\"All files ending with 'EO.edf' have been converted and saved in {output_folder}\")\n",
    "\n",
    "\n",
    "\n",
    "# Example usage\n",
    "input_folder = \"AllData/Figshare\"  # Replace with the actual path to the Figshare folder\n",
    "convert_ec_edf_to_fif_with_montage_and_scaling(input_folder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data preprocessing, trying to remove noise and leave real brain activities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import mne\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_fif_data(file_path):\n",
    "    raw_fif = mne.io.read_raw_fif(file_path, preload=True)\n",
    "    return raw_fif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downsampling(raw_fif):\n",
    "    target_sampling_rate = 250\n",
    "    \n",
    "    raw_downsampled = raw_fif.copy()  # Work on a copy to preserve the original object\n",
    "    raw_downsampled.resample(sfreq=target_sampling_rate, npad=\"auto\")\n",
    "    return raw_downsampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def highpass_lowpass_filter(raw_downsampled, l_freq=1.0, h_freq=45.0):\n",
    "    \n",
    "    raw_filtered = raw_downsampled.filter(\n",
    "        l_freq=l_freq, h_freq=h_freq, fir_design='firwin', phase='zero'\n",
    "    )\n",
    "    \n",
    "    return raw_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rereference_to_average(raw_filtered):\n",
    "    \n",
    "    raw_rereferenced = raw_filtered.set_eeg_reference(ref_channels='average', projection=False)\n",
    "    \n",
    "    return raw_rereferenced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auto_reject_bad_channel_and_interpolate(raw, flat_threshold=0.5, noise_threshold=500, verbose=True):\n",
    "    \"\"\"\n",
    "    Detects bad EEG channels based on flatness and excessive noise, interpolates them, and logs statistics.\n",
    "\n",
    "    Parameters:\n",
    "        raw (mne.io.Raw): The EEG Raw object.\n",
    "        flat_threshold (float): Minimum variance to flag a flat channel.\n",
    "        noise_threshold (float): Maximum variance to flag a noisy channel.\n",
    "        verbose (bool): If True, prints detailed information about bad channels.\n",
    "\n",
    "    Returns:\n",
    "        raw_cleaned (mne.io.Raw): The Raw object with bad channels interpolated.\n",
    "        stats (dict): Statistics about detected bad channels.\n",
    "    \"\"\"\n",
    "    # Get the raw data\n",
    "    data = raw.get_data()  # Shape: (n_channels, n_times)\n",
    "\n",
    "    # Detect flat channels (variance below threshold)\n",
    "    variances = np.var(data, axis=1)\n",
    "    flat_channels = [raw.ch_names[i] for i, var in enumerate(variances) if var < flat_threshold]\n",
    "\n",
    "    # Detect noisy channels (variance above threshold)\n",
    "    noisy_channels = [raw.ch_names[i] for i, var in enumerate(variances) if var > noise_threshold]\n",
    "\n",
    "    # Combine flat and noisy channels\n",
    "    bad_channels = list(set(flat_channels + noisy_channels))\n",
    "    raw.info['bads'] = bad_channels\n",
    "\n",
    "    # Log statistics\n",
    "    stats = {\n",
    "        \"total_channels\": len(raw.ch_names),\n",
    "        \"bad_channels\": len(bad_channels),\n",
    "        \"bad_channel_names\": bad_channels,\n",
    "        \"flat_channels\": flat_channels,\n",
    "        \"noisy_channels\": noisy_channels,\n",
    "    }\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Total channels: {stats['total_channels']}\")\n",
    "        print(f\"Number of bad channels detected: {stats['bad_channels']}\")\n",
    "        print(f\"Bad channel names: {stats['bad_channel_names']}\")\n",
    "        print(f\"Flat channels: {flat_channels}\")\n",
    "        print(f\"Noisy channels: {noisy_channels}\")\n",
    "\n",
    "    # Interpolate bad channels\n",
    "    raw_cleaned = raw.copy().interpolate_bads(reset_bads=True, mode='accurate')\n",
    "    if verbose:\n",
    "        print(\"Bad channels have been interpolated.\")\n",
    "\n",
    "    return raw_cleaned, stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_ica_and_reject(raw, amplitude_threshold=100, n_components=None, verbose=True):\n",
    "    \"\"\"\n",
    "    Performs ICA decomposition, rejects high-amplitude components, and provides rejection statistics,\n",
    "    including the percentage of variance explained by rejected components.\n",
    "\n",
    "    Parameters:\n",
    "        raw (mne.io.Raw): The preprocessed Raw EEG/MEG object.\n",
    "        amplitude_threshold (float): The peak-to-peak amplitude threshold to identify artifact components.\n",
    "        n_components (int or None): Number of ICA components to compute (default: None, auto-detect).\n",
    "        verbose (bool): If True, prints detailed information about the rejection process.\n",
    "\n",
    "    Returns:\n",
    "        raw_cleaned (mne.io.Raw): The Raw object after ICA and artifact rejection.\n",
    "        stats (dict): Statistics about the rejected ICA components, including variance explained.\n",
    "    \"\"\"\n",
    "    # Step 1: Perform ICA decomposition\n",
    "    ica = mne.preprocessing.ICA(n_components=n_components, random_state=97, max_iter=800)\n",
    "    ica.fit(raw)\n",
    "\n",
    "    # Step 2: Calculate the peak-to-peak amplitude of each ICA component\n",
    "    ica_sources = ica.get_sources(raw).get_data()  # Shape: (n_components, n_times)\n",
    "    ptp_amplitudes = np.ptp(ica_sources, axis=1)  # Peak-to-peak amplitude for each component\n",
    "\n",
    "    # Step 3: Identify high-amplitude components\n",
    "    high_amplitude_indices = [i for i, amp in enumerate(ptp_amplitudes) if amp > amplitude_threshold]\n",
    "\n",
    "    # Step 4: Calculate variance explained by each component\n",
    "    total_variance = np.var(raw.get_data())  # Total variance of the original data\n",
    "    component_variances = []\n",
    "    for i in range(len(ica_sources)):\n",
    "        # Reconstruct the signal for this single component\n",
    "        reconstructed_signal = np.outer(ica.mixing_matrix_[:, i], ica_sources[i])\n",
    "        component_variances.append(np.var(reconstructed_signal))\n",
    "\n",
    "    # Variance explained by rejected components\n",
    "    rejected_variance = sum([component_variances[i] for i in high_amplitude_indices])\n",
    "    percent_rejected_variance = (rejected_variance / total_variance) * 100\n",
    "\n",
    "    # Step 5: Exclude high-amplitude components\n",
    "    ica.exclude = high_amplitude_indices\n",
    "\n",
    "    # Step 6: Apply ICA to remove artifact components\n",
    "    raw_cleaned = ica.apply(raw)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"High-amplitude components rejected: {high_amplitude_indices}\")\n",
    "        print(f\"Percentage of variance explained by rejected components: {percent_rejected_variance:.2f}%\")\n",
    "\n",
    "    # Prepare statistics\n",
    "    stats = {\n",
    "        \"n_components\": len(ica_sources),\n",
    "        \"rejected_components\": len(high_amplitude_indices),\n",
    "        \"rejected_indices\": high_amplitude_indices,\n",
    "        \"variance_explained\": percent_rejected_variance,\n",
    "    }\n",
    "\n",
    "    return raw_cleaned, stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(raw_clean, file_path_to):\n",
    "    raw_clean.save(file_path_to, overwrite=True)\n",
    "    print(f'Cleaned EEG data saved to {file_path_to}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to loop over .fif files, apply preprocessing, save processed files,\n",
    "    and generate logs for rejected subjects, channels, and ICA components.\n",
    "    \"\"\"\n",
    "    # Define input and output folders\n",
    "    input_folder = \"AllData/Figshare/Converted_FIF_EO\"  # Replace with actual path to Converted_FIF\n",
    "    output_folder_name = \"Preprocessed_Figshare_EO\"\n",
    "    output_folder = os.path.join(os.path.dirname(input_folder), output_folder_name)\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # Define log file paths\n",
    "    rejected_subjects_log = os.path.join(output_folder, \"rejected_subjects.txt\")\n",
    "    rejected_channels_log = os.path.join(output_folder, \"rejected_channels.txt\")\n",
    "    rejected_ica_log = os.path.join(output_folder, \"rejected_components.txt\")\n",
    "    stats_log_file = os.path.join(output_folder, \"subject_stats.json\")\n",
    "\n",
    "    # Initialize logs\n",
    "    rejected_subjects = []\n",
    "    rejected_channels = {}\n",
    "    rejected_ica = {}\n",
    "    all_stats = {}\n",
    "\n",
    "    # Loop through all .fif files in the input folder\n",
    "    for file_name in os.listdir(input_folder):\n",
    "        if file_name.endswith(\".fif\"):  # Only process .fif files\n",
    "            input_file_path = os.path.join(input_folder, file_name)\n",
    "            output_file_path = os.path.join(output_folder, file_name)\n",
    "\n",
    "            print(f\"Processing file: {file_name}\")\n",
    "\n",
    "            # Load the .fif data\n",
    "            raw_fif = load_fif_data(input_file_path)\n",
    "\n",
    "            # Downsample the data to 250 Hz\n",
    "            raw_downsampled = downsampling(raw_fif)\n",
    "\n",
    "            # Apply bandpass filter (1–45 Hz)\n",
    "            raw_filtered = highpass_lowpass_filter(raw_downsampled, l_freq=1.0, h_freq=45.0)\n",
    "\n",
    "            # Automatically reject bad channels and interpolate\n",
    "            raw_good_channels, bad_channel_stats = auto_reject_bad_channel_and_interpolate(raw_filtered)\n",
    "\n",
    "            # Check the percentage of rejected channels\n",
    "            total_channels = bad_channel_stats[\"total_channels\"]\n",
    "            bad_channels = bad_channel_stats[\"bad_channels\"]\n",
    "            rejection_percentage = (bad_channels / total_channels) * 100\n",
    "\n",
    "            rejected_channels[file_name] = bad_channel_stats[\"bad_channel_names\"]\n",
    "            \n",
    "            if rejection_percentage > 20:\n",
    "                print(f\"Subject {file_name} rejected due to {rejection_percentage:.2f}% bad channels.\")\n",
    "                rejected_subjects.append(file_name)\n",
    "                all_stats[file_name] = {\"status\": \"rejected\", \"bad_channel_stats\": bad_channel_stats}\n",
    "                continue  # Skip further processing for this file\n",
    "\n",
    "            # Re-reference to average\n",
    "            raw_rereferenced = rereference_to_average(raw_good_channels)\n",
    "\n",
    "            # Perform ICA and reject high-amplitude components\n",
    "            raw_cleaned, ica_stats = perform_ica_and_reject(raw_rereferenced)\n",
    "\n",
    "            # Log rejected ICA components\n",
    "            rejected_ica[file_name] = {\n",
    "                \"rejected_components\": ica_stats[\"rejected_components\"],\n",
    "                \"variance_explained\": ica_stats[\"variance_explained\"]\n",
    "            }\n",
    "\n",
    "            # Save the cleaned data\n",
    "            save(raw_cleaned, output_file_path)\n",
    "\n",
    "            # Log stats for processed subjects\n",
    "            all_stats[file_name] = {\n",
    "                \"status\": \"processed\",\n",
    "                \"bad_channel_stats\": bad_channel_stats,\n",
    "                \"ica_stats\": ica_stats,\n",
    "            }\n",
    "\n",
    "            print(f\"Processed file saved to: {output_file_path}\")\n",
    "\n",
    "    # Write rejected subjects to a log file\n",
    "    with open(rejected_subjects_log, \"w\") as f:\n",
    "        for subject in rejected_subjects:\n",
    "            f.write(f\"{subject}\\n\")\n",
    "\n",
    "    # Write rejected channels to a log file\n",
    "    with open(rejected_channels_log, \"w\") as f:\n",
    "        for subject, channels in rejected_channels.items():\n",
    "            f.write(f\"{subject}: {channels}\\n\")\n",
    "\n",
    "    # Write rejected ICA components to a log file\n",
    "    with open(rejected_ica_log, \"w\") as f:\n",
    "        for subject, ica_info in rejected_ica.items():\n",
    "            f.write(f\"{subject}: {ica_info}\\n\")\n",
    "\n",
    "    # Write all stats to the JSON file\n",
    "    with open(stats_log_file, \"w\") as f:\n",
    "        json.dump(all_stats, f, indent=4)\n",
    "\n",
    "    print(f\"All files have been processed and saved in {output_folder}\")\n",
    "    print(f\"Rejected subjects logged in {rejected_subjects_log}\")\n",
    "    print(f\"Rejected channels logged in {rejected_channels_log}\")\n",
    "    print(f\"Rejected ICA components logged in {rejected_ica_log}\")\n",
    "    print(f\"All subject stats saved in {stats_log_file}\")\n",
    "\n",
    "# Call the main function\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize rejection log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "# Define the folder path where the logs are stored\n",
    "folder_path = \"AllData/Figshare/Processed_FIF\"\n",
    "\n",
    "# File paths for the logs\n",
    "rejected_subjects_file = os.path.join(folder_path, \"rejected_subjects.txt\")\n",
    "rejected_channels_file = os.path.join(folder_path, \"rejected_channels.txt\")\n",
    "rejected_components_file = os.path.join(folder_path, \"rejected_components.txt\")\n",
    "\n",
    "# Read the logs\n",
    "with open(rejected_subjects_file, 'r') as f:\n",
    "    rejected_subjects = f.readlines()\n",
    "\n",
    "with open(rejected_channels_file, 'r') as f:\n",
    "    rejected_channels = [line.strip() for line in f.readlines()]\n",
    "\n",
    "with open(rejected_components_file, 'r') as f:\n",
    "    rejected_components = [line.strip() for line in f.readlines()]\n",
    "\n",
    "# Initialize data storage for groups\n",
    "data = {\"H\": {\"subjects\": 0, \"channels\": [], \"independent components\": [], \"variance_explained\": []},\n",
    "        \"MDD\": {\"subjects\": 0, \"channels\": [], \"independent components\": [], \"variance_explained\": []}}\n",
    "\n",
    "# Process rejected subjects\n",
    "for subject in rejected_subjects:\n",
    "    group = \"H\" if subject.startswith(\"H\") else \"MDD\"\n",
    "    data[group][\"subjects\"] += 1\n",
    "\n",
    "# Process rejected channels\n",
    "for line in rejected_channels:\n",
    "    if line:\n",
    "        parts = line.split(\": \")\n",
    "        group = \"H\" if parts[0].startswith(\"H\") else \"MDD\"\n",
    "        channels_list = eval(parts[1]) if len(parts) > 1 and parts[1] else []\n",
    "        data[group][\"channels\"].append(len(channels_list))\n",
    "\n",
    "# Process rejected independent components\n",
    "for line in rejected_components:\n",
    "    if line:\n",
    "        parts = line.split(\": \", 1)\n",
    "        group = \"H\" if parts[0].startswith(\"H\") else \"MDD\"\n",
    "        stats_dict = eval(parts[1]) if len(parts) > 1 else {}\n",
    "        data[group][\"independent components\"].append(stats_dict.get('rejected_components', 0))\n",
    "        data[group][\"variance_explained\"].append(stats_dict.get('variance_explained', 0))\n",
    "\n",
    "# Calculate means and standard errors\n",
    "def calc_stats(data_list):\n",
    "    return np.mean(data_list), np.std(data_list) / np.sqrt(len(data_list))\n",
    "\n",
    "stats = {group: {\n",
    "    \"subjects\": data[group][\"subjects\"],\n",
    "    \"channels\": calc_stats(data[group][\"channels\"]),\n",
    "    \"independent components\": calc_stats(data[group][\"independent components\"]),\n",
    "    \"variance_explained\": calc_stats(data[group][\"variance_explained\"]),\n",
    "} for group in data.keys()}\n",
    "\n",
    "# Perform statistical tests\n",
    "p_channels = ttest_ind(data[\"H\"][\"channels\"], data[\"MDD\"][\"channels\"], equal_var=False).pvalue\n",
    "p_components = ttest_ind(data[\"H\"][\"independent components\"], data[\"MDD\"][\"independent components\"], equal_var=False).pvalue\n",
    "p_variance = ttest_ind(data[\"H\"][\"variance_explained\"], data[\"MDD\"][\"variance_explained\"], equal_var=False).pvalue\n",
    "\n",
    "# Apply Bonferroni correction\n",
    "p_values = [p_channels, p_components, p_variance]\n",
    "bonferroni_corrected = multipletests(p_values, method='bonferroni')[1]\n",
    "\n",
    "# Print corrected p-values\n",
    "print(\"Bonferroni Corrected p-values:\")\n",
    "print(f\"Channels: {bonferroni_corrected[0]:.3f}\")\n",
    "print(f\"Components: {bonferroni_corrected[1]:.3f}\")\n",
    "print(f\"Variance Explained: {bonferroni_corrected[2]:.3f}\")\n",
    "\n",
    "# Plot the results\n",
    "fig, axes = plt.subplots(1, 4, figsize=(22, 6))\n",
    "\n",
    "# Rejected Subjects\n",
    "axes[0].bar([\"H\", \"MDD\"], [stats[\"H\"][\"subjects\"], stats[\"MDD\"][\"subjects\"]])\n",
    "axes[0].set_title(\"Rejected Subjects\")\n",
    "axes[0].set_ylabel(\"Count\")\n",
    "\n",
    "# Rejected Channels\n",
    "axes[1].bar([\"H\", \"MDD\"], [stats[\"H\"][\"channels\"][0], stats[\"MDD\"][\"channels\"][0]],\n",
    "            yerr=[stats[\"H\"][\"channels\"][1], stats[\"MDD\"][\"channels\"][1]], capsize=5)\n",
    "axes[1].set_title(\"Rejected Channels\")\n",
    "axes[1].set_ylabel(\"Mean Count\")\n",
    "axes[1].text(0.5, max(stats[\"H\"][\"channels\"][0], stats[\"MDD\"][\"channels\"][0]),\n",
    "             f\"p={bonferroni_corrected[0]:.3f}\", ha='center')\n",
    "\n",
    "# Rejected Components\n",
    "axes[2].bar([\"H\", \"MDD\"], [stats[\"H\"][\"independent components\"][0], stats[\"MDD\"][\"independent components\"][0]],\n",
    "            yerr=[stats[\"H\"][\"independent components\"][1], stats[\"MDD\"][\"independent components\"][1]], capsize=5)\n",
    "axes[2].set_title(\"Rejected Independent Components\")\n",
    "axes[2].set_ylabel(\"Mean Count\")\n",
    "axes[2].text(0.5, max(stats[\"H\"][\"independent components\"][0], stats[\"MDD\"][\"independent components\"][0]),\n",
    "             f\"p={bonferroni_corrected[1]:.3f}\", ha='center')\n",
    "\n",
    "# Variance Explained by Rejected Components\n",
    "axes[3].bar([\"H\", \"MDD\"], [stats[\"H\"][\"variance_explained\"][0], stats[\"MDD\"][\"variance_explained\"][0]],\n",
    "            yerr=[stats[\"H\"][\"variance_explained\"][1], stats[\"MDD\"][\"variance_explained\"][1]], capsize=5)\n",
    "axes[3].set_title(\"Variance Explained by Rejected Independent Components\")\n",
    "axes[3].set_ylabel(\"Mean Variance (%)\")\n",
    "axes[3].text(0.5, max(stats[\"H\"][\"variance_explained\"][0], stats[\"MDD\"][\"variance_explained\"][0]),\n",
    "             f\"p={bonferroni_corrected[2]:.3f}\", ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
